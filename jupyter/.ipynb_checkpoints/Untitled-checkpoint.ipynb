{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wke18/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model successfully!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "corpus_path = r\"/home/wke18/Changgeng-Hospital/Data/embedding/Tencent_AILab_ChineseEmbedding.txt\"\n",
    "model = KeyedVectors.load_word2vec_format(corpus_path, binary=False)\n",
    "\n",
    "print(\"load model successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wke18/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.375991  0.177146  0.125519  0.137462  0.016419  0.101481  0.294055\n",
      "  0.172444  0.199107  0.469504  0.269921  0.091194 -0.154583 -0.131562\n",
      "  0.105564 -0.71419  -0.027736 -0.783807 -0.431038  0.161549  0.029468\n",
      " -0.377583  0.267362  0.015773 -0.249762  0.017412 -0.336516 -0.014074\n",
      "  0.131322  0.278532  0.101396 -0.203913  0.014773  0.036308 -0.024108\n",
      "  0.474032  0.264101  0.093336 -0.136918 -0.013357  0.154893 -0.101908\n",
      "  0.460812 -0.192273  0.170141 -0.325473  0.173993 -0.388985 -0.136157\n",
      "  0.160202 -0.437137 -0.004074 -0.422235  0.164338  0.263233  0.234079\n",
      "  0.501643  0.397155  0.256953  0.408039 -0.300643  0.448799 -0.030774\n",
      "  0.101205  0.088871  0.175874 -0.021934  0.10388   0.099399 -0.287805\n",
      " -0.247802  0.098585  0.110143  0.228435 -0.230434  0.139588  0.239408\n",
      "  0.183936  0.209531  0.072883 -0.005573 -0.083286 -0.24399   0.136201\n",
      "  0.381455 -0.69797  -0.272151 -0.043959 -0.277685 -0.432294 -0.049483\n",
      " -0.171292  0.055705 -0.155551  0.079304 -0.061461 -0.232251 -0.198808\n",
      " -0.309296 -0.176761  0.249036 -0.35132  -0.303918  0.290514 -0.083293\n",
      " -0.021462 -0.343947 -0.631541  0.116251 -0.26801   0.001954 -0.110827\n",
      "  0.215393 -0.303433  0.115061 -0.118986 -0.133085 -0.227598 -0.234729\n",
      " -0.163232  0.283852  0.131966 -0.189254 -0.146419  0.171444  0.396887\n",
      " -0.064674  0.104374 -0.338767  0.496565 -0.21551   0.05179   0.16783\n",
      "  0.145111 -0.309595 -0.292247 -0.270513 -0.391614  0.070806  0.48638\n",
      "  0.040083 -0.24658  -0.265049 -0.139738  0.226299  0.174939  0.320825\n",
      "  0.37785   0.200065  0.064847  0.471286 -0.185902  0.220168 -0.215521\n",
      " -0.509925  0.041705 -0.479067  0.235523  0.436463 -0.077088  0.055601\n",
      " -0.182865  0.154524  0.13663   0.544151 -0.064042 -0.200645  0.046486\n",
      "  0.119716 -0.242792 -0.371008  0.262575  0.136923  0.208193  0.364405\n",
      "  0.164465  0.04445  -0.362312 -0.298367 -0.124807 -0.102123 -0.709553\n",
      "  0.335873  0.244519 -0.188748  0.408577 -0.298648  0.105533  0.127051\n",
      " -0.074516 -0.071075  0.490988  0.101256 -0.105156  0.382213  0.358291\n",
      " -0.277473 -0.008881 -0.020505  0.322551]\n",
      "[('高脚杯', 0.8538842797279358), ('酒碗', 0.8522544503211975), ('端起酒杯', 0.8492069840431213), ('一饮而尽', 0.83536696434021), ('杯中的酒', 0.8284905552864075), ('杯子', 0.8228039145469666), ('酒盅', 0.8216980695724487), ('茶杯', 0.8192934989929199), ('空酒杯', 0.8174137473106384), ('酒樽', 0.8165823817253113)]\n"
     ]
    }
   ],
   "source": [
    "print(model.word_vec('地球'))\n",
    "print(model.most_similar('酒杯'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin working\n",
      "total word cnt = 30637\n",
      "[('：', 448532), ('/', 361872), ('。', 355512), ('；', 309169), ('无', 197686), ('、', 179063), (',', 177087), (')', 118730), ('(', 118497), ('次', 118264)]\n",
      "14329\n",
      "key size = 14329\n",
      "processing ..., cnt = 1000\n",
      "processing ..., cnt = 2000\n",
      "processing ..., cnt = 3000\n",
      "processing ..., cnt = 4000\n",
      "processing ..., cnt = 5000\n",
      "processing ..., cnt = 6000\n",
      "processing ..., cnt = 7000\n",
      "processing ..., cnt = 8000\n",
      "processing ..., cnt = 9000\n",
      "processing ..., cnt = 10000\n",
      "processing ..., cnt = 11000\n",
      "processing ..., cnt = 12000\n",
      "processing ..., cnt = 13000\n",
      "processing ..., cnt = 14000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jieba\n",
    "\n",
    "#得到训练集+测试集+测试集中atom的混合词表，先分开\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "word2id['PAD'], word2id['UNK'] = 0, 1\n",
    "id2word[0], id2word[1] = 'PAD', 'UNK'\n",
    "word_cnt = 2\n",
    "filtered_train_dir = r\"/data2/wk/data/pot/train_data_20\"\n",
    "filtered_valid_dir = r\"/data2/wk/data/pot/test_data_20\"\n",
    "\n",
    "print(\"begin working\")\n",
    "\n",
    "#训练集+测试集(删一下词的数量)\n",
    "def gen_word_list(train_data_dir = None, valid_data_dir = None):\n",
    "    #生成数据中的词集合\n",
    "    global word2id, id2word, word_cnt\n",
    "    word_dict = {}\n",
    "    train_file_list = os.listdir(train_data_dir)\n",
    "    valid_file_list = os.listdir(valid_data_dir)\n",
    "    file_list = train_file_list + valid_file_list\n",
    "    for file in file_list:\n",
    "        file = os.path.join(train_data_dir, file)\n",
    "        if not os.path.exists(file):\n",
    "            file = file.replace(train_data_dir, valid_data_dir)\n",
    "        data = json.load(open(file, \"r\"))\n",
    "        if \"note\" in data.keys():\n",
    "            data[\"text\"] = data[\"note\"]\n",
    "        text = jieba.lcut(data[\"text\"])\n",
    "        for word in text:\n",
    "            try:\n",
    "                temp = model[word]\n",
    "            except:\n",
    "                continue\n",
    "            if word not in word_dict.keys():\n",
    "                word_dict[word] = 0\n",
    "            word_dict[word] += 1\n",
    "    print(\"total word cnt = %d\" %len(word_dict))\n",
    "    sorted_word = sorted(word_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "    threthold = 5\n",
    "    print(sorted_word[:10])\n",
    "    for idx in range(len(sorted_word)):\n",
    "        if sorted_word[idx][1] < threthold:\n",
    "            break\n",
    "        word2id[sorted_word[idx][0]] = word_cnt\n",
    "        id2word[word_cnt] = sorted_word[idx][0]\n",
    "        word_cnt += 1\n",
    "    \n",
    "gen_word_list(filtered_train_dir, filtered_valid_dir)\n",
    "#total word cnt = 42112\n",
    "#threthold 20, idx = 9019\n",
    "#threthold 10, idx = 12542\n",
    "#threthold 5, idx = 17187\n",
    "\n",
    "\n",
    "neighbor_dict = {}\n",
    "def gen_similar_word_list(valid_data_dir = None):\n",
    "    #生成相似词的词典\n",
    "    global word2id, id2word, word_cnt\n",
    "    cnt = 0\n",
    "    prev_keys = []\n",
    "    print(len(word2id.keys()))\n",
    "    for key in word2id.keys():\n",
    "        prev_keys.append(key)\n",
    "    print(\"key size = %d\" %len(prev_keys))\n",
    "    for word in prev_keys:\n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            print(\"processing ..., cnt = %d\" %cnt)\n",
    "        if word in [\"PAD\", \"UNK\"]:\n",
    "            continue\n",
    "        if word in neighbor_dict.keys():\n",
    "            continue\n",
    "        neighbor_dict[word] = []\n",
    "        neighbor = model.most_similar(word, topn = 3)\n",
    "        for idx in range(3):\n",
    "            if neighbor[idx][0] not in word2id.keys():\n",
    "                word2id[neighbor[idx][0]] = word_cnt\n",
    "                id2word[word_cnt] = neighbor[idx][0]\n",
    "                word_cnt += 1\n",
    "            neighbor_dict[word].append(word2id[neighbor[idx][0]])\n",
    "\n",
    "#测试集atom\n",
    "gen_similar_word_list(filtered_valid_dir)\n",
    "\n",
    "#输出结果\n",
    "output_word2id_path = r\"/data2/wk/data/pot/dict/word2id.json\"\n",
    "output_id2word_path = r\"/data2/wk/data/pot/dict/id2word.json\"\n",
    "output_neighbor_path = r\"/data2/wk/data/pot/dict/neighbor.json\"\n",
    "\n",
    "json.dump(word2id, open(output_word2id_path, \"w\"))\n",
    "json.dump(id2word, open(output_id2word_path, \"w\"))\n",
    "json.dump(neighbor_dict, open(output_neighbor_path, \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进一步增添来自entity池的词\n",
    "entity_data_dir = r\"../newutil/mt_snomed.txt\"\n",
    "word2id_path = r\"/data2/wk/data/pot/dict/word2id.json\"\n",
    "id2word_path = r\"/data2/wk/data/pot/dict/id2word.json\"\n",
    "word2id = json.load(open(word2id_path, \"r\"))\n",
    "id2word = json.load(open(id2word_path, \"r\"))\n",
    "assert len(word2id) == len(id2word)\n",
    "word_cnt = len(word2id)\n",
    "\n",
    "def load_entity():\n",
    "    global word2id, id2word, word_cnt\n",
    "    f = open(entity_data_dir, \"r\", encoding=\"utf-8\")\n",
    "    cnt = 0\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(\"##\")\n",
    "        CUI = line[3]\n",
    "        title = line[1]\n",
    "        if title.find(\"(\") != -1:\n",
    "            title = title[:title.find(\"(\")]\n",
    "        if title.find(\"（\") != -1:\n",
    "            title = title[:title.find(\"（\")]\n",
    "        words = jieba.lcut(title)\n",
    "        for word in words:\n",
    "            try:\n",
    "                temp = model[word]\n",
    "            except:\n",
    "                continue\n",
    "            if word not in word2id.keys():\n",
    "                word2id[word] = word_cnt\n",
    "                id2word[word_cnt] = word\n",
    "                word_cnt += 1\n",
    "#添加entity所有词          \n",
    "load_entity()\n",
    "\n",
    "output_word2id_path = r\"/data2/wk/data/pot/dict/word2id_v2.json\"\n",
    "output_id2word_path = r\"/data2/wk/data/pot/dict/id2word_v2.json\"\n",
    "json.dump(word2id, open(output_word2id_path, \"w\"))\n",
    "json.dump(id2word, open(output_id2word_path, \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成词向量的pickle文件\n",
    "import numpy as np\n",
    "word2id_path = r\"/data2/wk/data/pot/dict/word2id_v2.json\"\n",
    "output_path = r\"/data2/wk/data/pot/dict/w2v.npy\"\n",
    "word2id = json.load(open(word2id_path, \"r\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
